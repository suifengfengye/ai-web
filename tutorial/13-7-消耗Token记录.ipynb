{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873a7e08-aa52-469e-bc52-f84beda13ee3",
   "metadata": {},
   "source": [
    "# 13.7 消耗Token记录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa815f-615a-4a3c-8dd3-99c74e456611",
   "metadata": {},
   "source": [
    "记录每次请求使用Token数量。\n",
    "\n",
    "场景：\n",
    "- 商业产品：WPS,购买了ZhipuAI的推理服务，会员可以使用。（多人共用一个API_KEY）\n",
    "- 企业内部，使用外部的LLM服务，根据Token记录每个人的花费（多人共用一个API_KEY）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec531d-1194-4153-bac3-346d357d7f03",
   "metadata": {},
   "source": [
    "### Decimal\n",
    "用来进行精确的十进制计算。\n",
    "- quantize(exp, rounding=None, context=None)\n",
    "\n",
    "银行家舍入法：\n",
    "\n",
    "1. “四”是指≤4时舍去，\"六\"是指≥6时进上\n",
    "2. 当5后有数时，舍5入1\n",
    "3. 当5后无有效数字时，5前为奇数，舍5入1\n",
    "4. 当5后无有效数字时，5前为偶数，舍5不进（0是偶数）。\n",
    "\n",
    "```python\n",
    "9.8249=9.82\n",
    "9.82671=9.83\n",
    "9.835=9.84\n",
    "9.8351 =9.84\n",
    "9.825=9.82\n",
    "9.82501=9.83\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "941310d7-8989-40a4-8b4b-7b369d45f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gpt-4o：\n",
    "提问：$2.50 / 1M input tokens -> 0.0025 $ / 1000 token -> 0.0025 * 7.2 = 0.018¥\n",
    "回答：$10.00 / 1M output tokens\n",
    "Pricing 批量API：\n",
    "提问：$1.25 / 1M input tokens\n",
    "回答：$5.00 / 1M output tokens\n",
    "\"\"\"\n",
    "from decimal import Decimal, ROUND_HALF_EVEN\n",
    "price_info = {\n",
    "    \"glm-4\": {\n",
    "        \"prompt_price\": Decimal(0.1), # 1000 token\n",
    "        \"completion_price\": Decimal(0.1),\n",
    "    }\n",
    "}\n",
    "# llm_output={'token_usage': {'completion_tokens': 554, 'prompt_tokens': 15, 'total_tokens': 569}, 'model_name': 'glm-4'}\n",
    "def compute_cost(llm_output):\n",
    "    if not llm_output:\n",
    "        raise Exception(\"llm_output为空\")\n",
    "    token_usage = llm_output.get(\"token_usage\", None)\n",
    "    if not token_usage:\n",
    "        raise Exception(\"token_usage为空\")\n",
    "    prompt_tokens = token_usage.get(\"prompt_tokens\", Decimal(0))\n",
    "    completion_tokens = token_usage.get(\"completion_tokens\", Decimal(0))\n",
    "\n",
    "    model_name = llm_output.get(\"model_name\", \"glm-4\")\n",
    "\n",
    "    _price_info = price_info.get(model_name)\n",
    "    prompt_price = _price_info.get(\"prompt_price\", Decimal(0))\n",
    "    completion_price = _price_info.get(\"completion_price\", Decimal(0))\n",
    "\n",
    "    prompt_cost = Decimal(prompt_tokens) / Decimal(1000) * prompt_price\n",
    "    completion_cost = Decimal(completion_tokens) / Decimal(1000) * completion_price\n",
    "\n",
    "    cost = prompt_cost + completion_cost\n",
    "    cost = cost.quantize(Decimal(\"0.0001\"),rounding=ROUND_HALF_EVEN)\n",
    "    return {\n",
    "        \"cost\": cost,\n",
    "        \"model_name\": model_name,\n",
    "        \"token_usage\": token_usage,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdaa7202-5f6e-466c-a1cf-aadc7b40c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, TypeVar, Union\n",
    "from uuid import UUID\n",
    "\n",
    "# LCEL: langchain expression language(langchain表达式语言)\n",
    "class LCELEventHandler(BaseCallbackHandler):  \n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        print(f\"on_llm_end,response: {response}, kwargs:{kwargs}\")\n",
    "        cost = compute_cost(response.llm_output)\n",
    "        print(f\"cost:{cost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "676dae45-f006-4db9-b69f-752f3164a917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入ZHIPUAI_API_KEY... ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LCELEventHandler.on_llm_end callback: Exception('llm_output为空')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_llm_end,response: generations=[[ChatGenerationChunk(text='LangChain是一个基于语言模型的人工智能框架，其事件体系主要是为了处理和响应在链中传递的各种事件。尽管“LangChain”一词在不同的上下文中可能有不同的含义，但一般来说，我们可以将事件体系描述为以下特点：\\n\\n1. **事件定义**：\\n   - 在LangChain中，事件可以理解为一系列预定义的动作或消息，这些动作或消息在链中的各个节点之间传递，触发相应的处理逻辑。\\n\\n2. **事件类型**：\\n   - **输入事件**：这是用户或其他系统发送到LangChain的初始请求或指令，它将启动整个链的运行。\\n   - **处理事件**：在链中流动的处理过程中触发的事件，可能包括数据处理、状态更新等。\\n   - **输出事件**：这是LangChain处理完毕后生成的结果或反馈，通常包括对用户请求的响应。\\n\\n3. **事件处理**：\\n   - LangChain的事件体系设计为能够处理各种类型的输入，并将其转化为有用的输出。\\n   - **节点处理**：在LangChain中，不同的节点可能负责处理不同类型的事件。每个节点在接收到事件后，根据预设的逻辑执行相关的操作。\\n\\n4. **事件流**：\\n   - 事件在LangChain中的流动形成了一个序列，每个事件都可能依赖于前一个事件的输出。\\n   - 这个序列通常是线性的，但也可以设计为有条件分支的结构，以处理复杂的逻辑。\\n\\n5. **错误处理**：\\n   - 在事件体系中，错误处理是一个关键组成部分。当事件处理失败时，LangChain应该有能力捕捉异常，并采取相应的措施，如重试、回退或报告错误。\\n\\n6. **状态管理**：\\n   - LangChain需要维护一个状态管理机制来跟踪事件处理的状态，确保信息的一致性和上下文的连续性。\\n\\n7. **集成和扩展**：\\n   - 事件体系还应该支持与外部系统集成，允许将LangChain的能力与其他系统或服务进行整合。\\n   - 同时，为了适应未来的需求，事件体系应具备扩展性，允许引入新的类型的事件和处理器。\\n\\n总的来说，LangChain的事件体系是为了保证信息的流动和处理的灵活性、可靠性和效率性，以适应复杂多变的业务场景和用户需求。它强调标准化和模块化，以便于系统的构建和维护。', generation_info={'finish_reason': 'stop'}, message=AIMessageChunk(content='LangChain是一个基于语言模型的人工智能框架，其事件体系主要是为了处理和响应在链中传递的各种事件。尽管“LangChain”一词在不同的上下文中可能有不同的含义，但一般来说，我们可以将事件体系描述为以下特点：\\n\\n1. **事件定义**：\\n   - 在LangChain中，事件可以理解为一系列预定义的动作或消息，这些动作或消息在链中的各个节点之间传递，触发相应的处理逻辑。\\n\\n2. **事件类型**：\\n   - **输入事件**：这是用户或其他系统发送到LangChain的初始请求或指令，它将启动整个链的运行。\\n   - **处理事件**：在链中流动的处理过程中触发的事件，可能包括数据处理、状态更新等。\\n   - **输出事件**：这是LangChain处理完毕后生成的结果或反馈，通常包括对用户请求的响应。\\n\\n3. **事件处理**：\\n   - LangChain的事件体系设计为能够处理各种类型的输入，并将其转化为有用的输出。\\n   - **节点处理**：在LangChain中，不同的节点可能负责处理不同类型的事件。每个节点在接收到事件后，根据预设的逻辑执行相关的操作。\\n\\n4. **事件流**：\\n   - 事件在LangChain中的流动形成了一个序列，每个事件都可能依赖于前一个事件的输出。\\n   - 这个序列通常是线性的，但也可以设计为有条件分支的结构，以处理复杂的逻辑。\\n\\n5. **错误处理**：\\n   - 在事件体系中，错误处理是一个关键组成部分。当事件处理失败时，LangChain应该有能力捕捉异常，并采取相应的措施，如重试、回退或报告错误。\\n\\n6. **状态管理**：\\n   - LangChain需要维护一个状态管理机制来跟踪事件处理的状态，确保信息的一致性和上下文的连续性。\\n\\n7. **集成和扩展**：\\n   - 事件体系还应该支持与外部系统集成，允许将LangChain的能力与其他系统或服务进行整合。\\n   - 同时，为了适应未来的需求，事件体系应具备扩展性，允许引入新的类型的事件和处理器。\\n\\n总的来说，LangChain的事件体系是为了保证信息的流动和处理的灵活性、可靠性和效率性，以适应复杂多变的业务场景和用户需求。它强调标准化和模块化，以便于系统的构建和维护。', response_metadata={'finish_reason': 'stop'}, id='run-0fd7ffb7-bcc3-4098-8cca-e9a0ab680c05'))]] llm_output=None run=None, kwargs:{'run_id': UUID('0fd7ffb7-bcc3-4098-8cca-e9a0ab680c05'), 'parent_run_id': UUID('bce7ad58-da2d-4b39-84f3-f2c7d5605611'), 'tags': ['seq:step:2']}\n"
     ]
    }
   ],
   "source": [
    "# ChatPromptTempalte\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama, ChatZhipuAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"ZHIPUAI_API_KEY\"] =  getpass.getpass(\"请输入ZHIPUAI_API_KEY...\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"请你使用中文描述一下LangChain的事件体系？\")\n",
    "\n",
    "\n",
    "# 1. 创建llm对象\n",
    "# chat_model = ChatOllama(model=\"llama3\")\n",
    "chat_model = ChatZhipuAI(model=\"glm-4\")\n",
    "\n",
    "# 2. 创建一个langchain对象\n",
    "parser = StrOutputParser()\n",
    "chain = (prompt.with_config({\"run_name\":\"prompt_dxh\"}) \n",
    "| chat_model.with_config({\"run_name\":\"chat_model_dxh\"}) \n",
    "| parser.with_config({\"run_name\":\"parser_dxh\"}) )\n",
    "\n",
    "chain = chain.with_config({\"callbacks\": [LCELEventHandler()], \"run_name\": \"chain_dxh\"})\n",
    "\n",
    "# 1. invoke()调用\n",
    "# result = chain.invoke({},{\"callbacks\": [LCELEventHandler()]})\n",
    "# 2. ainvoke()调用\n",
    "# result = await chain.ainvoke({})\n",
    "# 3. stream()调用\n",
    "# result = chain.stream({})\n",
    "# for chunk in result:\n",
    "#     ...\n",
    "# 4. astream()调用\n",
    "result = chain.astream({})\n",
    "async for chunk in result:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d55562-b990-4689-9903-aa7fafb7cf62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
