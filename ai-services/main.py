from fastapi import FastAPI, Path, Query, Body
import uvicorn
from typing import Annotated
from enum import Enum
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_models import ChatOllama
from fastapi.responses import StreamingResponse
from langchain_core.runnables.history import RunnableWithMessageHistory
from sqlalchemy import create_engine
from sqlalchemy.ext.asyncio import create_async_engine
from langchain_community.chat_message_histories import SQLChatMessageHistory
from langchain_core.prompts import MessagesPlaceholder

# uvicorn APP [OPTIONS]
# uvicorn "<module>:<attribute>" [OPTIONS]
app = FastAPI()


class OpTypeEnum(Enum):
    empty = ""
    polish = "polish"
    continue_writing = "continue_writing"
    shorten = "shorten"
    expand = "expand"


class OpSubTypeEnum(Enum):
    empty = ""
    colloquial = "colloquial"
    lively = "lively"
    formal = "formal"


class AIDocBodyType(BaseModel):
    op_type: OpTypeEnum = Field(default=OpTypeEnum.empty, title="æ“ä½œç±»å‹")
    op_sub_type: OpSubTypeEnum = Field(
        default=OpSubTypeEnum.empty, title="æ¶¦è‰²çš„å­ç±»å‹")
    question: str = Field(default="", description="è‡ªç”±é—®é¢˜")
    content: str = ""


def get_session_history(session_id: str):
    return SQLChatMessageHistory(session_id,
                                 connection=create_engine("sqlite:///./db/memory.db"))


def aget_session_history(session_id: str):
    return SQLChatMessageHistory(session_id,
                                 connection=create_async_engine(
                                     "sqlite+aiosqlite:///./db/memory.db"),
                                 async_mode=True)


@app.post("/ai_docs/generate")
async def generate_ai_docs(params: AIDocBodyType = Body(title="è¯·æ±‚ä½“ä¿¡æ¯")):
    """
        1. æ¶¦è‰² (polish)
          1.1 å£è¯­åŒ– (colloquial)s
          1.2 æ›´æ´»æ³¼ (lively)
          1.3 æ›´æ­£å¼ (formal)
        2. ç»­å†™ (continue_writing)
        3. ç¼©çŸ­ç¯‡å¹… (shorten)
        4. æ‰©å……ç¯‡å¹… (expand)
      """
    # 1. ç¼–å†™SystemPrompt
    system_prompt_text = ("ä½ æ˜¯ä¸€ä½è‘—åçš„ä½œå®¶ï¼Œåå­—å«åš'è´¹å°V'ã€‚"
                          "å¦‚æœé—®ä½ 'ä½ æ˜¯è°',è¯·ä¸è¦å›ç­”ä»»ä½•å…¶ä»–å†…å®¹ï¼Œç›´æ¥å›ç­”'è´¹å°V'å³å¯ã€‚"
                          "ç°åœ¨çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·å°†æ–‡ç« çš„å†…å®¹è¿›è¡Œå¤„ç†ï¼Œç»­å†™æ–‡ç« ã€ç¼©çŸ­æ–‡ç« ç¯‡å¹…æˆ–è€…æ‰©å……æ–‡ç« ç¯‡å¹…ã€‚")
    # 2. æ„å»ºpromptå¯¹è±¡
    prompt = ChatPromptTemplate.from_messages([
        ('system', system_prompt_text),
        MessagesPlaceholder(variable_name="history"),
        ('human', "{input}")
    ])
    # 3. åˆ›å»ºllmå¯¹è±¡
    llm = ChatOllama(model="llama3")
    # 4. æ„å»ºlangchainå¯¹è±¡
    chain = prompt | llm
    # chain_history = RunnableWithMessageHistory(
    #     chain,
    #     get_session_history=get_session_history,
    #     history_messages_key="history"
    # )
    achain_history = RunnableWithMessageHistory(
        chain,
        get_session_history=aget_session_history,
        history_messages_key="history"
    )
    # 5. ç¼–å†™ä¸šåŠ¡é€»è¾‘
    input = ""
    if params.question:
        if params.content:
            input = ("æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œå¯¹æ–‡ç« å†…å®¹è¿›è¡Œå¤„ç†ã€‚"
                     f"é—®é¢˜='{params.question}'"
                     f"æ–‡ç« å†…å®¹='{params.content}")
        else:
            input = params.question
    else:
        action_dict = {
            "polish": "æ¶¦è‰²",
            "continue_writing": "ç»­å†™",
            "shorten": "ç¼©çŸ­ç¯‡å¹…",
            "expand": "æ‰©å……ç¯‡å¹…",
        }
        sub_action_dict = {
            "colloquial": "æ›´å£è¯­åŒ–",
            "lively": "æ›´æ´»æ³¼",
            "formal": "æ›´æ­£å¼",
        }
        op_type_zh = action_dict[params.op_type.value]
        action = f"{op_type_zh}å¤„ç†"
        if params.op_type == OpTypeEnum.polish:
            op_sub_type_zh = sub_action_dict[params.op_sub_type.value]
            action = f"æ¶¦è‰²å¤„ç†,è®©æ–‡ç« å†…å®¹{op_sub_type_zh}"
        input = f"è¯·å¯¹ä¸‹é¢çš„æ–‡ç« å†…å®¹è¿›è¡Œ{action}:{params.content}"

    # 1 invoke()è°ƒç”¨
    # result = chain_history.invoke({
    #     "input": input
    # }, {"configurable": {"session_id": "daxiaohan_01"}})
    # return result
    """
    {"input": input}
    {"configurable": {"session_id": "daxiaohan_01"}} 
    ğŸ‘‡
    {"input": input, "history": [SystemMessage(content=""),HumanMessage(content=""),...]}
    ğŸ‘‡
    prompt ğŸ‘‰ llm
    
    """

    # 2 stream()è°ƒç”¨
    # result = chain.stream({
    #   "input": input
    # })
    # # AIMessageChunk(content="xxxx")
    # def stream_response(result):
    #   # yield 'xxxxx'
    #   for chunk in result:
    #     yield chunk.content
    # return StreamingResponse(stream_response(result), media_type="text/plain")

    # 3. ainvoke()è°ƒç”¨
    # result = await chain.ainvoke({
    #   "input": input
    # })
    # return result

    # 4. astream()è°ƒç”¨
    result = achain_history.astream({
        "input": input
    }, {"configurable": {"session_id": "daxiaohan_01"}})
    # AIMessageChunk(content="xxxx")

    async def astream_response(result):
        # yield 'xxxxx'
        async for chunk in result:
            yield chunk.content
    return StreamingResponse(astream_response(result), media_type="text/plain")


# python fastapi_demo.py
if __name__ == '__main__':
    uvicorn.run('main:app', host="127.0.0.1", port=8081, reload=True)
