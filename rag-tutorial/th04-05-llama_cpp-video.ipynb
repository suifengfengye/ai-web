{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6b0feca-71bc-4496-a092-0533c4fa5656",
   "metadata": {},
   "source": [
    "# 1. llama.cpp\n",
    "\n",
    "创始人：Georgi Gerganov。\n",
    "\n",
    "llama.cpp是一个github上的开源项目。\n",
    "它的主要目标是在各种硬件上（本地和云端）以最少的设置实现大语言模型推理，并达到最先进的性能。\n",
    "\n",
    "- Ollama的底层就是llama.cpp这个项目。Ollama更侧重用户交互，而llama.cpp更侧重推理的性能。\n",
    "    - ollama安装更方便。\n",
    "    - 提供了 [model library在线仓库](https://ollama.com/library) 。\n",
    "- 底层语言：C/C++。\n",
    "- 模型文件格式要求为GGUF（文件后缀为.gguf）。（huggingface上的模型通常是 PyTorch(后缀为.pt/.pth) 或 SafeTensors 格式(后缀为.safetensors)）\n",
    "    - GGUF (GGML Universal File), [来源](https://pypi.org/project/gguf/) \n",
    "        -  ~~GGUF 是 \"GPT-Generated Unified Format\" 的缩写，中文名称为：“GPT 生成的统一格式”。~~\n",
    "    -  GGUF 是一种专为LLM 模型设计的二进制文件格式，由开源项目llama.cpp 的创始人Georgi Gerganov 提出。\n",
    "    -  GGUF格式侧重于提升模型加载和推理过程中的性能和效率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b37df3-e7f2-4249-9f6d-4713fb5a7845",
   "metadata": {},
   "source": [
    "## 1.1 llama.cpp的安装\n",
    "\n",
    "- macos\n",
    "    ```shell\n",
    "        brew install llama.cpp\n",
    "    ```\n",
    "    \n",
    "- docker镜像，[地址](https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md)\n",
    "\n",
    "    - ghcr.io/ggml-org/llama.cpp:full, 包含了llama.cpp的所有功能，大小2G\n",
    "    - ghcr.io/ggml-org/llama.cpp:light, 只包含主可执行文件,大小为94M\n",
    "    - ghcr.io/ggml-org/llama.cpp:server，只包含server的可执行文件,大小96M\n",
    "    - ......\n",
    "  \n",
    "- 预编译好的文件，[地址](https://github.com/ggml-org/llama.cpp/releases) ,windows等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc611c7c-b744-44a4-82b2-a209e818ea13",
   "metadata": {},
   "source": [
    "## 1.2 命令行和转换工具\n",
    "\n",
    "- llama-cli: 一个用于访问和试验大部分 llama.cpp 功能的命令行界面工具。\n",
    "- **llama-server**: 一个轻量级的，OpenAI API兼容的用于服务大语言模型的 HTTP 服务器。\n",
    "- llama-bench: 对各种参数下的推理性能进行基准测试。\n",
    "- llama-run: 在命令行中运行LLM，和ollama run 一样。\n",
    "- llama-sample: 根据参数运行一次，运行结束后退出。（对开发人员比较有用）\n",
    "\n",
    "格式转换工具（将其他模型文件转换成gguf格式）\n",
    "\n",
    "- convert_hf_to_gguf.py\n",
    "    -  将 Hugging Face 格式的模型权重（通常是 PyTorch 或 SafeTensors 格式）转换为 GGUF 格式。\n",
    "    \n",
    "- convert_hf_to_gguf_update.py\n",
    "    - 用于更新convert_hf_to_gguf.py工具中的get_vocab_base_pre()函数\n",
    "    - get_vocab_base_pre(): 根据分词器获取模型类型的一个函数。\n",
    "    - 一般不需要执行这个工具，除非遇到下面的错误：\n",
    "        - NotImplementedError(\"BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\")\n",
    "\n",
    "- convert_llama_ggml_to_gguf.py\n",
    "    - 将旧版的 GGML 格式模型文件转换为新版的 GGUF 格式。\n",
    "    - GGML文件格式，它是gguf的前身，也是llama.cpp的作者提出。随着llama.cpp项目的演进和发展，这种格式后续会慢慢被替换掉。\n",
    "    - ~~GGML，General GPU Management Layer，通用GPU管理层。~~\n",
    "    - ggml 是 Georgi Gerganov Machine Learning 的缩写，其命名来源于项目的核心开发者 Georgi Gerganov（llama.cpp 和 ggml 库的主要作者）\n",
    "\n",
    "- convert_lora_to_gguf.py\n",
    "    - 将 LoRA（Low-Rank Adaptation）微调模型转换为 GGUF 格式，使其可以与基础模型结合并在 llama.cpp 中运行。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a7dc3-4abc-49cf-8c5b-8b9c402dc292",
   "metadata": {},
   "source": [
    "## 1.3 llama-server\n",
    "\n",
    "- -m: m是model的缩写。\n",
    "\n",
    "- --host：指定主机，可以是0.0.0.0（监听所有可用的网络接口，包括localhost）,也可以是localhost，默认为localhost/127.0.0.1。\n",
    "\n",
    "- --port：指定服务端口，默认端口为8080\n",
    "\n",
    "- --embedding：指定服务于embedding模型\n",
    "\n",
    "- --reranking: 指定服务于reranker模型\n",
    "\n",
    "- --pooling：指定池化类型，可选值none/mean/cls/last/rank。\n",
    "    \n",
    "    - 一种将多个token嵌入向量合并成一个固定大小向量的技术。\n",
    "\n",
    "\n",
    "```shell\n",
    "llama-server -m bge-reranker-large-f16.gguf --port 11433  --reranking --pooling rank\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8bb0a-af3a-4489-936f-f2eabac1ba28",
   "metadata": {},
   "source": [
    "# 2. 实现一个langchain的压缩器\n",
    "\n",
    "压缩器，用于将检索器检索到的文档列表进行后处理。langchain对压缩器封装的基类为BaseDocumentCompressor，所以压缩器更准确的称呼为“文档压缩器”。\n",
    "\n",
    "可以参考：CrossEncoderReranker 的实现。\n",
    "\n",
    "- BaseDocumentCompressor\n",
    "    - compress_documents(): 抽象方法，需要手动实现。\n",
    "      1. 传入问题，和文档列表。\n",
    "      2. 使用requests库，请求llama.cpp启动的reranker服务，得到相似度得分列表。\n",
    "      3. 根据返回的得分对文档列表进行重排序，将排序后的文档列表返回。\n",
    "\n",
    "\n",
    "    - acompress_documents(): 非抽象方法，默认即可；也可以自己实现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85508b3-f397-4276-9066-bd3195ab6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl http://127.0.0.1:8012/v1/rerank \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"some-model\",\n",
    "            \"query\": \"What is panda?\",\n",
    "            \"top_n\": 3,\n",
    "            \"documents\": [\n",
    "                \"hi\",\n",
    "            \"it is a bear\",\n",
    "            \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n",
    "            ]\n",
    "    }' | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bfb0c41-65d6-400e-979a-4b4cc4b91b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "from langchain_core.callbacks import Callbacks\n",
    "from langchain_core.documents import BaseDocumentCompressor, Document\n",
    "\n",
    "from langchain.retrievers.document_compressors.cross_encoder import BaseCrossEncoder\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "class LlamaCppReranker(BaseDocumentCompressor):\n",
    "\n",
    "    url: str = \"http://localhost:8080/reranking\"\n",
    "    \n",
    "    top_n: int = 3\n",
    "    \"\"\"Number of documents to return.\"\"\"\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "        callbacks: Optional[Callbacks] = None,\n",
    "    ) -> Sequence[Document]:\n",
    "        payload = {\n",
    "            # \"model\": \"大小寒学AI\",\n",
    "            \"query\": query,\n",
    "            \"top_n\": self.top_n,\n",
    "            \"documents\":[item.page_content for item in documents]\n",
    "        }\n",
    "        response = requests.post(self.url, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            response_json = response.json()\n",
    "            print(response_json)\n",
    "            scores = response_json.get(\"results\", [])\n",
    "            scores = [score_item.get(\"relevance_score\") for score_item in scores]\n",
    "            # return []\n",
    "            docs_with_scores = list(zip(documents, scores))\n",
    "            # lambda x:x[1]\n",
    "            result = sorted(docs_with_scores, key=operator.itemgetter(1), reverse=True)\n",
    "            return [doc for doc, _ in result[: self.top_n]]\n",
    "        else:\n",
    "            raise Excpetion(\"Failed to compress_documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d926b39b-87f5-4b6f-93ca-2a540fea0e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': '大小寒学AI', 'object': 'list', 'usage': {'prompt_tokens': 3074, 'total_tokens': 3074}, 'results': [{'index': 0, 'relevance_score': -1.235522747039795}, {'index': 1, 'relevance_score': -3.671241521835327}, {'index': 2, 'relevance_score': -5.785731792449951}, {'index': 3, 'relevance_score': -5.383221626281738}, {'index': 4, 'relevance_score': -1.5460096597671509}, {'index': 5, 'relevance_score': -7.980041027069092}, {'index': 6, 'relevance_score': -6.460325717926025}, {'index': 7, 'relevance_score': -8.532350540161133}, {'index': 8, 'relevance_score': -8.52607536315918}, {'index': 9, 'relevance_score': -6.12175989151001}, {'index': 10, 'relevance_score': -7.699027061462402}, {'index': 11, 'relevance_score': -4.303559303283691}, {'index': 12, 'relevance_score': -7.3094000816345215}, {'index': 13, 'relevance_score': -9.055702209472656}, {'index': 14, 'relevance_score': -7.169929504394531}, {'index': 15, 'relevance_score': -6.98286247253418}, {'index': 16, 'relevance_score': -6.237729072570801}, {'index': 17, 'relevance_score': 3.1915855407714844}, {'index': 18, 'relevance_score': -8.246188163757324}, {'index': 19, 'relevance_score': -7.591038227081299}]}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "query = \"2024年中东非智能手机的出货量为多少？\"\n",
    "\n",
    "chromadb_client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "embedding_fn = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "        \n",
    "vector_store = Chroma(collection_name=\"rag-video-collection\",\n",
    "                              client=chromadb_client, \n",
    "                              embedding_function=embedding_fn\n",
    "                            )\n",
    "retriever = vector_store.as_retriever(\n",
    "            # search_type=\"mmr\",\n",
    "            # search_kwargs={\"k\": 16, \"fetch_k\": 20, \"lambda_mult\": 0.1}\n",
    "            search_kwargs={\"k\": 20}\n",
    "        )\n",
    "\n",
    "# Document对象,id/page_content/metadata\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "reranker = LlamaCppReranker(\n",
    "    url=\"http://localhost:11433/reranking\",\n",
    "    top_n=4\n",
    ")\n",
    "results = reranker.compress_documents(\n",
    "    query=query,\n",
    "    documents=docs,\n",
    ")\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd7e8454-7d70-44cb-aa62-a2a7bc6ce7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/rag-video/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1. 添加文档到chromadb当中\n",
    "# 2. 根据问题进行检索并输出答案\n",
    "\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "def get_metadata_str(metadata):\n",
    "    if metadata is None:\n",
    "        return \"\"\n",
    "    _str = \"\"\n",
    "    # (key, value)\n",
    "    for key, value in metadata.items():\n",
    "        _str += f\"{key}:{value}\\n\"\n",
    "    return _str\n",
    "\n",
    "class RagLangChain():\n",
    "    def __init__(self, \n",
    "                 collection_name: str = \"rag-video-collection\", \n",
    "                 host: str = \"localhost\",\n",
    "                port: int = 8000):\n",
    "        self.collection_name = collection_name\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "\n",
    "    def __get_vector_store(self):\n",
    "        chromadb_client = chromadb.HttpClient(host=self.host, port=self.port)\n",
    "        embedding_fn = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "        \n",
    "        vector_store = Chroma(collection_name=self.collection_name,\n",
    "                              client=chromadb_client, \n",
    "                              embedding_function=embedding_fn\n",
    "                             )\n",
    "        return vector_store\n",
    "\n",
    "    def delete_collection(self):\n",
    "        vector_store = self.__get_vector_store()\n",
    "        vector_store.delete_collection()\n",
    "        \n",
    "    def add_file(self, file_path: str, metadata = None):\n",
    "        loader = TextLoader(file_path)\n",
    "        \n",
    "        docs = loader.load()\n",
    "        text_spliter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "        all_splits = text_spliter.split_documents(docs)\n",
    "\n",
    "        if metadata is not None:\n",
    "            for split_item in all_splits:\n",
    "                split_metadata = split_item.metadata\n",
    "                split_item.metadata = {**split_metadata, **metadata}\n",
    "        \n",
    "        vector_store = self.__get_vector_store()\n",
    "        ids = vector_store.add_documents(documents=all_splits)\n",
    "        return ids\n",
    "\n",
    "    def __query_vector(self, info):\n",
    "        vector_store = self.__get_vector_store()\n",
    "        retriever = vector_store.as_retriever(\n",
    "            # search_type=\"mmr\",\n",
    "            # search_kwargs={\"k\": 16, \"fetch_k\": 20, \"lambda_mult\": 0.1}\n",
    "            search_kwargs={\"k\": 20}\n",
    "        )\n",
    "\n",
    "        # Document对象,id/page_content/metadata\n",
    "        # docs = retriever.invoke(info[\"query\"])\n",
    "\n",
    "        # model = CrossEncoder(\"BAAI/bge-reranker-large\")\n",
    "        # # question = \"2024年中东非智能手机的出货量为多少？\"\n",
    "        # scores = model.predict([(info[\"query\"], doc_item.page_content) for doc_item in docs])\n",
    "        # # [(score, Document), (score, Document), ...]\n",
    "        # sorted_list = sorted(zip(scores, docs), reverse=True, key=lambda x:x[0])\n",
    "        # docs = [doc for _, doc in sorted_list]\n",
    "        # docs = docs[:4]\n",
    "\n",
    "\n",
    "        # 2. 使用langchain的CrossEncoder进行重排\n",
    "        # model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-large\")\n",
    "        # compressor = CrossEncoderReranker(\n",
    "        #     model=model,\n",
    "        #     top_n=4,\n",
    "        # )\n",
    "        compressor = LlamaCppReranker(\n",
    "            url=\"http://localhost:11433/reranking\",\n",
    "            top_n=4\n",
    "        )\n",
    "        compressor_retriever = ContextualCompressionRetriever(\n",
    "            base_retriever=retriever,\n",
    "            base_compressor=compressor,\n",
    "        )\n",
    "\n",
    "        docs = compressor_retriever.invoke(info[\"query\"])\n",
    "        \n",
    "        # (0, item0), (1, item1)\n",
    "        for index, doc_item in enumerate(docs):\n",
    "            print(f\"文档片段{index}\")\n",
    "            print(doc_item)\n",
    "            print(\"*\" * 80)\n",
    "        # docs_str = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        docs_str = \"\\n\\n\".join(f\"\"\"\n",
    "{get_metadata_str(doc.metadata)}\n",
    "{doc.page_content}\"\"\" for doc in docs)\n",
    "        # print(f\"@@@@@{docs_str}\")\n",
    "        return docs_str\n",
    "        \n",
    "    def query(self, question: str):\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "你是一个问答机器人。你的任务是根据下述给定的已知信息回答用户问题。\n",
    "\n",
    "已知信息:\n",
    "{context}\n",
    "用户问题：\n",
    "{query}\n",
    "如果已知信息不包含用户问题的答案，或者已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
    "请不要输出已知信息中不包含的信息或答案。\n",
    "请用中文回答用户问题。\n",
    "\"\"\")\n",
    "        \n",
    "        llm = ChatOllama(model=\"qwen2.5:latest\")\n",
    "        \n",
    "        output_parser = StrOutputParser()\n",
    "        # 返回值: {\"query\": \"XXXX\", \"context\": \"XXXXX2\"}\n",
    "        chain = ( {\"context\": self.__query_vector, \"query\": lambda x: x[\"query\"]} | prompt | llm | output_parser)\n",
    "        \n",
    "        result = chain.invoke({\"query\": question})\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "039da8af-9529-476f-9d19-d24b8c9a6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = RagLangChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15aa4015-6533-4fde-b9d3-b089745210c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': '大小寒学AI', 'object': 'list', 'usage': {'prompt_tokens': 3074, 'total_tokens': 3074}, 'results': [{'index': 0, 'relevance_score': -1.235522747039795}, {'index': 1, 'relevance_score': -3.671241521835327}, {'index': 2, 'relevance_score': -5.785731792449951}, {'index': 3, 'relevance_score': -5.383221626281738}, {'index': 4, 'relevance_score': -1.5460096597671509}, {'index': 5, 'relevance_score': -7.980041027069092}, {'index': 6, 'relevance_score': -6.460325717926025}, {'index': 7, 'relevance_score': -8.532350540161133}, {'index': 8, 'relevance_score': -8.52607536315918}, {'index': 9, 'relevance_score': -6.12175989151001}, {'index': 10, 'relevance_score': -7.699027061462402}, {'index': 11, 'relevance_score': -4.303559303283691}, {'index': 12, 'relevance_score': -7.3094000816345215}, {'index': 13, 'relevance_score': -9.055702209472656}, {'index': 14, 'relevance_score': -7.169929504394531}, {'index': 15, 'relevance_score': -6.98286247253418}, {'index': 16, 'relevance_score': -6.237729072570801}, {'index': 17, 'relevance_score': 3.1915855407714844}, {'index': 18, 'relevance_score': -8.246188163757324}, {'index': 19, 'relevance_score': -7.591038227081299}]}\n",
      "文档片段0\n",
      "page_content='从地区来看，中东非的出货量为 4,199 万部，同比增速为 14%，表现亮眼， 唯一一个达到双位数增速的地区。紧随其后的亚洲其他地区出货量4,502 万 部，同比增长8%。这两个区域的出货量表现好于我们此前预测。虽然印度 三季度的出货量同比增长5%，但是略微低于我们此前预测。\n",
      "在三季度，中国的出货量 6,878 万部，同比增长 3%，环比下滑 4%(图表32)。欧洲和北美发达市场的出货量分别同比下降 5%和 1%。这三个区域的 出货量表现大体符合我们此前的预测。' metadata={'source': '科技行业 2025 年展望.txt', '作者': '沈岱,马智焱,黄佳琦', '发表时间': '2024 年 12 月 13 日', '文章标题': '科技行业2025年展望'}\n",
      "********************************************************************************\n",
      "文档片段1\n",
      "page_content='三、智能手机:2024 年、2025 年连续两年保持增长\n",
      "预计 2025 年全球智能手机出货量将同比增长 2%\n",
      "根据 IDC 数据，今年三季度全球智能手机出货量 3.15 亿部，环比增长 8%， 同比增长4%(图表 34)。虽然同比增速较一二季度下行，但是同比增速略 好于我们此前预测。这与我们与智能手机供应链沟通下来的情况类似，比半 年报看到的行业需求情况略好。' metadata={'source': '科技行业 2025 年展望.txt', '作者': '沈岱,马智焱,黄佳琦', '发表时间': '2024 年 12 月 13 日', '文章标题': '科技行业2025年展望'}\n",
      "********************************************************************************\n",
      "文档片段2\n",
      "page_content='高端机型相对优异的表现与我们在安卓品牌端以及供应链端看到的情况类 似。虽然全球智能手机换机周期仍然处于偏高位置，但是在较长换机周期的 情况下，用户愿意提升预算购买相对高端的机型。而千元以下手机占比提升， 主要来自于中东非等发展地区手机出货量更加强劲的表现。这可能是美国进 入降息周期带动这些地区手机需求的释放。\n",
      "其中，小米的高端机型表现比较典型。今年三季度，小米 3,000 元人民币以 上的智能手机出货量占到小米总出货量的6.3%，同比增加 1.5 个百分点，环 比增加0.5 个百分点(图表 41)。' metadata={'source': '科技行业 2025 年展望.txt', '作者': '沈岱,马智焱,黄佳琦', '发表时间': '2024 年 12 月 13 日', '文章标题': '科技行业2025年展望'}\n",
      "********************************************************************************\n",
      "文档片段3\n",
      "page_content='总体来看，我们预期今年全球智能手机出货量将达到 12.2 亿部，同比增长5.1%。我们略微上调全年出货量预测。全球智能手机出货量在今年取得较好 增长，在2023 年接近冰点的出货量基础上实现了较好的复苏。\n",
      "我们认为 2025 年有望延续 2024 年的复苏势头，预计 2025 年全球智能手机 出货量将达到12.4 亿部，同比增长 1.6%。由于今年一二季度全球智能手机 出货量接近2 位数的增长，2025 年上半年智能手机出货量可能会录得同比 下滑。但是，明年同比增速有望呈现前低后高的态势，从而带来全年出货量 的正增长(图表30)。' metadata={'source': '科技行业 2025 年展望.txt', '作者': '沈岱,马智焱,黄佳琦', '发表时间': '2024 年 12 月 13 日', '文章标题': '科技行业2025年展望'}\n",
      "********************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2024年中东非智能手机的出货量为4,199万部。'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.query(question=\"2024年中东非智能手机的出货量为多少？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e73dfc-d727-49ce-944b-30ddb6fe6ce9",
   "metadata": {},
   "source": [
    "# 3. llama.cpp的gguf转换工具\n",
    "\n",
    "格式转换工具（将其他模型文件转换成gguf格式）\n",
    "\n",
    "- 安装说明\n",
    "    - 下载llama.cpp项目到本地\n",
    "    - 使用conda创建一个虚拟环境，如\"gguf-env\"\n",
    "      ```shell\n",
    "      conda create -n gguf-env python=3.10\n",
    "      ```\n",
    "    - 在llama.cpp目录下执行 pip install  -r requirements.txt\n",
    "    - 【注意】不要单独将这些py工具文件和requirements.txt/requirements目录单独出去进行使用，对最新的gguf-py有依赖。\n",
    "\n",
    "- convert_hf_to_gguf.py\n",
    "    -  将 Hugging Face 格式的模型权重（通常是 PyTorch 或 SafeTensors 格式）转换为 GGUF 格式。\n",
    "\n",
    "\n",
    "```shell\n",
    "# outtype, choices=[\"f32\", \"f16\", \"bf16\", \"q8_0\", \"tq1_0\", \"tq2_0\", \"auto\"], default=\"f16\"\n",
    "python convert_hf_to_gguf.py ./models/Qwen2.5-1.5B-Instruct --outtype q8_0 --outfile ./models/Qwen2.5-1.5B-Instruct_q8_0.gguf\n",
    "```\n",
    "    \n",
    "- convert_hf_to_gguf_update.py\n",
    "    - 用于更新convert_hf_to_gguf.py工具中的get_vocab_base_pre()函数\n",
    "    - get_vocab_base_pre(): 根据分词器获取模型类型的一个函数。\n",
    "    - 一般不需要执行这个工具，除非遇到下面的错误：\n",
    "        - NotImplementedError(\"BPE pre-tokenizer was not recognized - update get_vocab_base_pre()\")\n",
    "    - 执行这个工具时会遇到的错误：\n",
    "        - ValueError: Unrecognized model in models/tokenizers/command-r.\n",
    "        - ValueError: Unrecognized model in models/tokenizers/jais.\n",
    "        - ValueError: Unrecognized model in models/tokenizers/tekken.\n",
    "        - ValueError: Unrecognized model in models/tokenizers/exaone.\n",
    "        - 是因为这些分词器对应的模型，访问需要申请权限。从大陆网段申请/注册的账号为大陆的话，可能会被拒绝。 如果不使用这些模型的话，可以将convert_hf_to_gguf_update.py中models配置中对应的配置注释掉就可以了。\n",
    "\n",
    "```\n",
    " python convert_hf_to_gguf_update.py <huggingface_token>\n",
    "```\n",
    "\n",
    "- convert_llama_ggml_to_gguf.py\n",
    "    - 将旧版的 GGML 格式模型文件转换为新版的 GGUF 格式。\n",
    "    - GGML文件格式，它是gguf的前身，也是llama.cpp的作者提出。随着llama.cpp项目的演进和发展，这种格式后续会慢慢被替换掉。\n",
    "    - ggml 是 Georgi Gerganov Machine Learning 的缩写，其命名来源于项目的核心开发者 Georgi Gerganov（llama.cpp 和 ggml 库的主要作者）\n",
    "\n",
    "- convert_lora_to_gguf.py\n",
    "    - 将 LoRA（Low-Rank Adaptation）微调模型转换为 GGUF 格式，使其可以与基础模型结合并在 llama.cpp 中运行。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d341c9a1-0802-427e-97f6-f4e380eb4d85",
   "metadata": {},
   "source": [
    "## 3.1 ollama中使用自己的gguf模型\n",
    "\n",
    "1. Modelfile文件\n",
    "创建一个Modelfile文件，并写入如下内容.\n",
    "\n",
    "注意将\"./vicuna-33b.Q4_0.gguf\"改成自己的gguf文件路径。\n",
    "\n",
    "```\n",
    "FROM ./vicuna-33b.Q4_0.gguf\n",
    "```\n",
    "\n",
    "2. 创建ollama模型\n",
    "\n",
    "注意：Modelfile为第1步中创建的Modelfile文件。\n",
    "\n",
    "命令为：\n",
    "\n",
    "```shell\n",
    "# example为模型的名字，可自行修改\n",
    "ollama create example -f Modelfile\n",
    "```\n",
    "3. 在命令行中运行\n",
    "\n",
    "```\n",
    "ollama run example\n",
    "```\n",
    "\n",
    "4. 推送自己的模型到ollama仓库\n",
    "\n",
    "![ollama_push](./ollama_push.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72afe712-9185-4069-8a24-d64dd28b22dc",
   "metadata": {},
   "source": [
    "## 3.2 git lfs 插件\n",
    "\n",
    "lfs：large file storage,大文件存储。\n",
    "\n",
    "它是一个git的插件，安装命令为：\n",
    "\n",
    "1. 安装\n",
    "\n",
    "[官网地址](https://git-lfs.com/)\n",
    "\n",
    "    1.1 windows\n",
    "\n",
    "\n",
    "    从 https://gitforwindows.org/ 下载安装\n",
    "\n",
    "\n",
    "    1.2 macOS\n",
    "\n",
    "```shell\n",
    "brew install git-lfs\n",
    "```\n",
    "\n",
    "    1.3 linux\n",
    "\n",
    "```shell\n",
    "# apt/deb: \n",
    "sudo apt-get install git-lfs\n",
    "#yum/rpm: \n",
    "sudo yum install git-lfs\n",
    "```\n",
    "\n",
    "2. 初始化\n",
    "\n",
    "```shell\n",
    "# 2. 初始化\n",
    "git lfs install\n",
    "```\n",
    "\n",
    "3. 验证\n",
    "\n",
    "```shell\n",
    "# 1. 验证是否安装成功\n",
    "git lfs --help\n",
    "# 2. 查看当前git目录下哪些文件是通过git lfs下载的\n",
    "git lfs ls-files\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87159c-5fcb-4cac-a702-99392b778900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
